nohup: ignoring input
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[61682,1],0] (PID 15515)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
[th-es-ln0:08718] 2 more processes have sent help message help-opal-runtime.txt / opal_init:warn-fork
[th-es-ln0:08718] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
imagenet train dataset
imagenet test dataset
train.py, svd_rank = 3
I am the master: the world size is 3, cur step: 1
imagenet train dataset
imagenet test dataset
train.py, svd_rank = 3
I am worker: 2 in all 2 workers, next step: 0
Worker 2: starting training
/THL5/home/daodao/anaconda2/envs/pytorch1.1.0/lib/python2.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean = torch.tensor(mean, dtype=torch.float32)
/THL5/home/daodao/anaconda2/envs/pytorch1.1.0/lib/python2.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std = torch.tensor(std, dtype=torch.float32)
imagenet train dataset
imagenet test dataset
train.py, svd_rank = 3
I am worker: 1 in all 2 workers, next step: 0
Worker 1: starting training
Master node is entering step: 1
Rank of this node: 2, Current step: 1
/THL5/home/daodao/anaconda2/envs/pytorch1.1.0/lib/python2.7/site-packages/torchvision/transforms/functional.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean = torch.tensor(mean, dtype=torch.float32)
/THL5/home/daodao/anaconda2/envs/pytorch1.1.0/lib/python2.7/site-packages/torchvision/transforms/functional.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std = torch.tensor(std, dtype=torch.float32)
Rank of this node: 1, Current step: 1
[96, 98]
[96, 98]
[gn7:21317] *** Process received signal ***
[gn7:21317] Signal: Segmentation fault (11)
[gn7:21317] Signal code: Invalid permissions (2)
[gn7:21317] Failing at address: 0x208d290
[gn8:01900] *** Process received signal ***
[gn8:01900] Signal: Segmentation fault (11)
[gn8:01900] Signal code: Invalid permissions (2)
[gn8:01900] Failing at address: 0xc73290
[gn7:21317] [ 0] /usr/lib64/libpthread.so.0(+0xf100)[0x2b1160005100]
[gn7:21317] [ 1] [0x208d290]
[gn7:21317] *** End of error message ***
[gn8:01900] [ 0] /usr/lib64/libpthread.so.0(+0xf100)[0x2acc00fe7100]
[gn8:01900] [ 1] [0xc73290]
[gn8:01900] *** End of error message ***
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 1 with PID 21317 on node gn7 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
2 total processes killed (some possibly by mpirun during cleanup)
